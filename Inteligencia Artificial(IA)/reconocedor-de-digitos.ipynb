{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "709f9c41",
   "metadata": {
    "papermill": {
     "duration": 0.003899,
     "end_time": "2025-08-15T16:28:27.289714",
     "exception": false,
     "start_time": "2025-08-15T16:28:27.285815",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1. Importamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fcfb1a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T16:28:27.298003Z",
     "iopub.status.busy": "2025-08-15T16:28:27.297674Z",
     "iopub.status.idle": "2025-08-15T16:28:41.955778Z",
     "shell.execute_reply": "2025-08-15T16:28:41.954812Z"
    },
    "papermill": {
     "duration": 14.664335,
     "end_time": "2025-08-15T16:28:41.957645",
     "exception": false,
     "start_time": "2025-08-15T16:28:27.293310",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69ccb3dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T16:28:41.966043Z",
     "iopub.status.busy": "2025-08-15T16:28:41.965595Z",
     "iopub.status.idle": "2025-08-15T16:28:41.973755Z",
     "shell.execute_reply": "2025-08-15T16:28:41.972741Z"
    },
    "papermill": {
     "duration": 0.014063,
     "end_time": "2025-08-15T16:28:41.975277",
     "exception": false,
     "start_time": "2025-08-15T16:28:41.961214",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1c53ac",
   "metadata": {
    "papermill": {
     "duration": 0.002934,
     "end_time": "2025-08-15T16:28:41.981704",
     "exception": false,
     "start_time": "2025-08-15T16:28:41.978770",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2. Procesamiento de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bc95a46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T16:28:41.989433Z",
     "iopub.status.busy": "2025-08-15T16:28:41.989057Z",
     "iopub.status.idle": "2025-08-15T16:28:41.997314Z",
     "shell.execute_reply": "2025-08-15T16:28:41.996213Z"
    },
    "papermill": {
     "duration": 0.01429,
     "end_time": "2025-08-15T16:28:41.999098",
     "exception": false,
     "start_time": "2025-08-15T16:28:41.984808",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DigitRecognizerDataset(Dataset):\n",
    "    # train Indica si el dataset es de entrenamiento o de prueba.\n",
    "    # Si train=True, la clase separa labels (etiquetas) de las imágenes.\n",
    "    # Si train=False, significa que son datos de prueba y no hay etiquetas, solo imágenes\n",
    "    \n",
    "    def __init__(self, csv_file,transform=None,train=True):\n",
    "        self.data=pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        self.train = train\n",
    "        if self.train:\n",
    "            self.labels=self.data['label'].values\n",
    "            self.images = self.data.drop(columns=['label']).values\n",
    "\n",
    "        else:\n",
    "            self.labels = None\n",
    "            self.images = self.data.values\n",
    "\n",
    "        # Reorganizamos las imágenes y las preparamos para la red neuronal\n",
    "        self.images = (\n",
    "            self.images\n",
    "            .reshape(-1, 28, 28, 1)  # Cambia la forma: número de imágenes x 28 x 28 x 1 canal (blanco y negro)\n",
    "            .astype(np.float32)        # Convierte los valores de píxeles a float32 para cálculos más precisos\n",
    "            / 255.0                    # Normaliza los valores a rango [0,1] para que la red entrene mejor\n",
    "        )\n",
    "\n",
    "    # __len__ define cuántos elementos tiene el dataset.\n",
    "    # Esto es necesario para que PyTorch sepa cuántos ejemplos hay y pueda iterar sobre ellos.\n",
    "    def __len__(self):\n",
    "        return len(self.images)  # Devuelve el número de imágenes en el dataset\n",
    "\n",
    "    # __getitem__ define cómo obtener un solo ejemplo del dataset por su índice idx.\n",
    "    # Esto es esencial para el DataLoader, que pide ejemplos de uno en uno o en batch.\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]  # Obtiene la imagen en la posición idx\n",
    "\n",
    "        # Aplica transformaciones si se han definido (como normalización o data augmentation)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Si estamos en modo entrenamiento, devolvemos también la etiqueta correspondiente\n",
    "        if self.train:\n",
    "            label = self.labels[idx]  # Obtiene la etiqueta para esa imagen\n",
    "            return image, label      # Devuelve un tuple (imagen, etiqueta)\n",
    "    \n",
    "        # Si no estamos entrenando (por ejemplo test set), solo devolvemos la imagen\n",
    "        return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cec6b6c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T16:28:42.006977Z",
     "iopub.status.busy": "2025-08-15T16:28:42.006645Z",
     "iopub.status.idle": "2025-08-15T16:28:47.811112Z",
     "shell.execute_reply": "2025-08-15T16:28:47.809969Z"
    },
    "papermill": {
     "duration": 5.810773,
     "end_time": "2025-08-15T16:28:47.813051",
     "exception": false,
     "start_time": "2025-08-15T16:28:42.002278",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Definimos las transformaciones para las imágenes de entrenamiento\n",
    "# Compose permite encadenar varias transformaciones\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),         # Convierte la imagen de numpy array a objeto PIL para poder aplicar transformaciones\n",
    "    transforms.RandomRotation(10),   # Rota la imagen aleatoriamente hasta 10 grados (data augmentation)\n",
    "    transforms.ToTensor(),           # Convierte la imagen PIL a tensor de PyTorch (valores entre 0 y 1)\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normaliza los valores del tensor para que tengan media 0.5 y desviación 0.5\n",
    "])\n",
    "\n",
    "# Transformaciones para las imágenes de test\n",
    "# Aquí no aplicamos data augmentation, solo convertimos a tensor y normalizamos\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Creamos el dataset de entrenamiento usando nuestra clase personalizada\n",
    "# Le pasamos el CSV con las imágenes, las transformaciones y le indicamos que es train=True\n",
    "train_dataset = DigitRecognizerDataset('/kaggle/input/digit-recognizer/train.csv',\n",
    "                                       transform=train_transform,\n",
    "                                       train=True)\n",
    "\n",
    "# Creamos el dataset de test\n",
    "# Aquí train=False porque no tenemos labels y no aplicamos data augmentation\n",
    "test_dataset = DigitRecognizerDataset('/kaggle/input/digit-recognizer/test.csv',\n",
    "                                      transform=test_transform,\n",
    "                                      train=False)\n",
    "\n",
    "# Creamos el DataLoader de entrenamiento\n",
    "# batch_size=64 -> cada batch tendrá 64 imágenes\n",
    "# shuffle=True -> mezcla las imágenes en cada epoch para mejorar el entrenamiento\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# DataLoader para test\n",
    "# shuffle=False -> no mezclamos, mantenemos el orden de las imágenes\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc5a6bd",
   "metadata": {
    "papermill": {
     "duration": 0.003354,
     "end_time": "2025-08-15T16:28:47.820196",
     "exception": false,
     "start_time": "2025-08-15T16:28:47.816842",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# definimos modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3632d4ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T16:28:47.828721Z",
     "iopub.status.busy": "2025-08-15T16:28:47.828345Z",
     "iopub.status.idle": "2025-08-15T16:28:47.846891Z",
     "shell.execute_reply": "2025-08-15T16:28:47.845972Z"
    },
    "papermill": {
     "duration": 0.024682,
     "end_time": "2025-08-15T16:28:47.848671",
     "exception": false,
     "start_time": "2025-08-15T16:28:47.823989",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Definimos la clase de la CNN\n",
    "class SimpleCNN(nn.Module):\n",
    "    \n",
    "    # __init__ se ejecuta cuando creamos una instancia del modelo\n",
    "    # Aquí definimos todas las capas que va a usar la red\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(SimpleCNN, self).__init__()  # Llama al constructor de nn.Module\n",
    "        \n",
    "        # Primera capa convolucional: 1 canal de entrada (grayscale), 16 filtros, kernel 3x3\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # Segunda capa convolucional: 16 canales de entrada, 32 filtros\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # Capa de max pooling: reduce el tamaño de la imagen a la mitad\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Capa totalmente conectada: de 32*7*7 neuronas a 128\n",
    "        self.fc1 = nn.Linear(32 * 7 * 7, 128)\n",
    "        \n",
    "        # Capa de salida: de 128 neuronas a num_classes (10 dígitos)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "        \n",
    "        # Dropout: apaga aleatoriamente el 25% de neuronas para evitar overfitting\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "    # forward define cómo pasan los datos por la red\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # conv1 -> ReLU -> pooling\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # conv2 -> ReLU -> pooling\n",
    "        x = x.view(-1, 32 * 7 * 7)            # aplana la imagen para la capa densa\n",
    "        x = F.relu(self.fc1(x))               # fc1 + ReLU\n",
    "        x = self.dropout(x)                    # aplica dropout\n",
    "        x = self.fc2(x)                        # capa de salida\n",
    "        return x\n",
    "\n",
    "# Creamos el modelo y lo movemos a la GPU si está disponible\n",
    "model = SimpleCNN().to(device)\n",
    "\n",
    "# Definimos la función de pérdida (cross entropy, típica para clasificación)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Definimos el optimizador (Adam) con learning rate de 0.001\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdafd19a",
   "metadata": {
    "papermill": {
     "duration": 0.002901,
     "end_time": "2025-08-15T16:28:47.854979",
     "exception": false,
     "start_time": "2025-08-15T16:28:47.852078",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## entrenamos modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "debfe54b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T16:28:47.862482Z",
     "iopub.status.busy": "2025-08-15T16:28:47.862091Z",
     "iopub.status.idle": "2025-08-15T16:28:47.868337Z",
     "shell.execute_reply": "2025-08-15T16:28:47.867458Z"
    },
    "papermill": {
     "duration": 0.01185,
     "end_time": "2025-08-15T16:28:47.869927",
     "exception": false,
     "start_time": "2025-08-15T16:28:47.858077",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(epochs=50):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss=0.0\n",
    "        for i,(images,labels) in enumerate(train_loader):\n",
    "            images,labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs=model(images)\n",
    "            loss=criterion(outputs,labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            if i % 200 == 199:\n",
    "                print(f\"[Epoch {epoch + 1}, Batch {i + 1}] Loss: {running_loss / 200:.3f}\")\n",
    "                running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43306c9e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T16:28:47.878597Z",
     "iopub.status.busy": "2025-08-15T16:28:47.877767Z",
     "iopub.status.idle": "2025-08-15T16:28:47.884016Z",
     "shell.execute_reply": "2025-08-15T16:28:47.882933Z"
    },
    "papermill": {
     "duration": 0.012777,
     "end_time": "2025-08-15T16:28:47.885905",
     "exception": false,
     "start_time": "2025-08-15T16:28:47.873128",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_model():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(f\"Validation Accuracy: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af1f32b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T16:28:47.894523Z",
     "iopub.status.busy": "2025-08-15T16:28:47.893487Z",
     "iopub.status.idle": "2025-08-15T16:33:52.745456Z",
     "shell.execute_reply": "2025-08-15T16:33:52.744305Z"
    },
    "papermill": {
     "duration": 304.86224,
     "end_time": "2025-08-15T16:33:52.751435",
     "exception": false,
     "start_time": "2025-08-15T16:28:47.889195",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1, Batch 200] Loss: 0.701\n",
      "[Epoch 1, Batch 400] Loss: 0.207\n",
      "[Epoch 1, Batch 600] Loss: 0.160\n",
      "[Epoch 2, Batch 200] Loss: 0.109\n",
      "[Epoch 2, Batch 400] Loss: 0.108\n",
      "[Epoch 2, Batch 600] Loss: 0.093\n",
      "[Epoch 3, Batch 200] Loss: 0.078\n",
      "[Epoch 3, Batch 400] Loss: 0.079\n",
      "[Epoch 3, Batch 600] Loss: 0.073\n",
      "[Epoch 4, Batch 200] Loss: 0.067\n",
      "[Epoch 4, Batch 400] Loss: 0.071\n",
      "[Epoch 4, Batch 600] Loss: 0.066\n",
      "[Epoch 5, Batch 200] Loss: 0.058\n",
      "[Epoch 5, Batch 400] Loss: 0.058\n",
      "[Epoch 5, Batch 600] Loss: 0.057\n",
      "[Epoch 6, Batch 200] Loss: 0.046\n",
      "[Epoch 6, Batch 400] Loss: 0.052\n",
      "[Epoch 6, Batch 600] Loss: 0.050\n",
      "[Epoch 7, Batch 200] Loss: 0.046\n",
      "[Epoch 7, Batch 400] Loss: 0.046\n",
      "[Epoch 7, Batch 600] Loss: 0.045\n",
      "[Epoch 8, Batch 200] Loss: 0.037\n",
      "[Epoch 8, Batch 400] Loss: 0.036\n",
      "[Epoch 8, Batch 600] Loss: 0.048\n",
      "[Epoch 9, Batch 200] Loss: 0.039\n",
      "[Epoch 9, Batch 400] Loss: 0.039\n",
      "[Epoch 9, Batch 600] Loss: 0.036\n",
      "[Epoch 10, Batch 200] Loss: 0.030\n",
      "[Epoch 10, Batch 400] Loss: 0.035\n",
      "[Epoch 10, Batch 600] Loss: 0.033\n",
      "Validation Accuracy: 99.40%\n"
     ]
    }
   ],
   "source": [
    "train_model(epochs=10)\n",
    "evaluate_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5e8857",
   "metadata": {
    "papermill": {
     "duration": 0.004698,
     "end_time": "2025-08-15T16:33:52.761965",
     "exception": false,
     "start_time": "2025-08-15T16:33:52.757267",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Predict Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "150953b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T16:33:52.772383Z",
     "iopub.status.busy": "2025-08-15T16:33:52.772039Z",
     "iopub.status.idle": "2025-08-15T16:33:52.778865Z",
     "shell.execute_reply": "2025-08-15T16:33:52.777725Z"
    },
    "papermill": {
     "duration": 0.014108,
     "end_time": "2025-08-15T16:33:52.780593",
     "exception": false,
     "start_time": "2025-08-15T16:33:52.766485",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_test():\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for images in test_loader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            predictions.extend(predicted.cpu().numpy())\n",
    "    \n",
    "    submission = pd.DataFrame({\n",
    "        'ImageId': range(1, len(predictions) + 1),\n",
    "        'Label': predictions\n",
    "    })\n",
    "    submission.to_csv('submission.csv', index=False)\n",
    "    print(\"Predictions saved to submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4762950c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T16:33:52.791892Z",
     "iopub.status.busy": "2025-08-15T16:33:52.791176Z",
     "iopub.status.idle": "2025-08-15T16:33:58.679772Z",
     "shell.execute_reply": "2025-08-15T16:33:58.678703Z"
    },
    "papermill": {
     "duration": 5.895902,
     "end_time": "2025-08-15T16:33:58.681490",
     "exception": false,
     "start_time": "2025-08-15T16:33:52.785588",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to submission.csv\n"
     ]
    }
   ],
   "source": [
    "predict_test()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 861823,
     "sourceId": 3004,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 339.430169,
   "end_time": "2025-08-15T16:34:01.566331",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-08-15T16:28:22.136162",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
